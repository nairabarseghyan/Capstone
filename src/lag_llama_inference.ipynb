{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "<h1 style=\"text-align: center;\">Lag-Llama model inference</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1:** Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n",
      "Using MPS device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nairabarseghyan/anaconda3/lib/python3.11/site-packages/google/colab/data_table.py:30: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  from IPython.utils import traitlets as _traitlets\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time \n",
    "import torch\n",
    "\n",
    "\n",
    "from data_prep import load_and_preprocess_data, general_revenue_dataframe, split_univariate_data, get_lag_llama_dataset\n",
    "from config import MODEL_PATH, find_device\n",
    "from models import lag_llama_estiamtor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2 Set up lag-llama environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "%cd lag-llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.3 import lag-llama libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lag_llama.gluon.estimator import LagLlamaEstimator\n",
    "from matplotlib import pyplot as plt\n",
    "from gluonts.evaluation import Evaluator, make_evaluation_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2:**  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data and adding necessary features\n",
    "initial_data = load_and_preprocess_data()\n",
    "univariate_data = general_revenue_dataframe(initial_data)\n",
    "\n",
    "train_data_raw, test_data_raw = split_univariate_data(univariate_data)\n",
    "\n",
    "train_data = get_lag_llama_dataset(train_data_raw,frequency = \"1D\")\n",
    "test_data = get_lag_llama_dataset(test_data_raw,frequency = \"1D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n"
     ]
    }
   ],
   "source": [
    "prediction_length = len(test_data_raw)  # prediction length\n",
    "num_samples = 1000 # sampled from the distribution for each timestep\n",
    "llama_device = torch.device(find_device())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3:**  Importing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_llama_model_filename = os.path.join(MODEL_PATH, 'fine_tuned_lag_llama_model.pth')\n",
    "\n",
    "# Load the predictor from the file\n",
    "with open(lag_llama_model_filename, 'rb') as f:\n",
    "    fine_tuned_predictor = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 4:**  Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\" >================    ZERO SHOT PREDICTION    ================</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satrt_time_zs = time.time()\n",
    "\n",
    "forecasts_ZS_val, tss_ZS_val = get_lag_llama_predictions(\n",
    "    test_data, prediction_length, num_samples\n",
    ")\n",
    "print(\"Zero - Shot inference completed in --- %s seconds ---\" % (time.time() - satrt_time_zs))\n",
    "\n",
    "predction_zero_shots_df = transfrom_lag_llama_predictions(forecasts_ZS_val, tss_ZS_val, prediction_length)\n",
    "\n",
    "predction_zero_shots_df.to_csv(f\"{DATA_PATH}results/Predictions/lag_llama_zero_shot.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\" > ================    FINE TUNED MODEL PREDICTION   ================</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satrt_time_ft = time.time()\n",
    "\n",
    "forecasts_FT_val, tss_FT_val = get_lag_llama_predictions(\n",
    "    test_data, prediction_length, num_samples, predictor = fine_tuned_predictor\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Inference of Fine_tuned completed in --- %s seconds ---\" % (time.time() - satrt_time_ft))\n",
    "\n",
    "predction_fine_tune_df = transfrom_lag_llama_predictions(forecasts_FT_val, tss_FT_val, prediction_length)\n",
    "\n",
    "predction_fine_tune_df.to_csv(f\"{DATA_PATH}results/Predictions/lag_llama_fine_tune.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 5:**  Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5.1 Evaluate zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_ZS_train, tss_ZS_train = get_lag_llama_predictions(\n",
    "    train_data, prediction_length, num_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_lag_llama_model(tss_ZS_train, forecasts_ZS_train, tss_ZS_val, forecasts_ZS_val,model_name = 'zero_shot_lag_llama')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5.1 Evaluate Fine Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_FT_train, tss_FT_train = get_lag_llama_predictions(\n",
    "    train_data, prediction_length, num_samples, predictor = fine_tuned_predictor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_lag_llama_model(tss_FT_train, forecasts_FT_train, tss_FT_val, forecasts_FT_val,model_name = 'fine_tuned_lag_llama')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
